最近遇到了一个bug，偶尔出来一次，就是rr建联失败，把排查过程需要用到的知识记录下来。
现象是有时候会出现SYN-RECEIVED，按道理这个状态是一个瞬间状态，同一个连接看见一次算你运气好，看见多次，说明
你需要看代码了。
我首先确定下，ss -t打印的State字段，到底是指什么。rpm -qf `which ss`确定rpm包，rpm信息显示，ss命令归属于
iproute工具集。

下载源码。
源码显示，ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。
tcp_diag是一个用于分析统计的模块，可以获得Linux 内核中socket信息，是一位俄罗斯人写的。
ss比netstat快，net-tools 就是要用来被iproute2淘汰的。
用途	net-tool（被淘汰）	iproute2
地址和链路配置	ifconfig	ip addr, ip link
路由表	route	           ip route
邻居	arp	            ip neigh
VLAN	vconfig	             ip link
隧道	iptunnel	    ip tunnel
组播	ipmaddr	            ip maddr
统计	netstat	            ss
以上是背景-------------------------
linux内核中使用tcp4_seq_show 来读取/proc/net/tcp 和 /proc/net/tcp6文件的，ss 通过PF_FILE的socket与内核通信，获取
当前的socket状态。
RFC793中关于SYN_RECV状态的描述如下：
SYN-RECEIVED - represents waiting for a confirming connection
request acknowledgment after having both received and sent a
connection request.
但是linux并不是这样实现的，而是稍微做了修改，这个是当时非常难以理解的地方。
syn_recv的状态，是关于连接socket的，不是关于监听socket的，最初的监听socket的状态是listen状态，
那么连接socket是什么时候建立，SYN-RECEIVED状态又是如何来设置以及迁移的呢？
tcp_v4_do_rcv函数中，
if (sk->sk_state == TCP_LISTEN) {
	struct sock *nsk = tcp_v4_hnd_req(sk, skb);/*会调用tcp_check_req，然后回调tcp_v4_syn_recv_sock*/
		if (!nsk)
			goto discard;

		if (nsk != sk) {/*第一次收到syn报文，是不走这个流程，因为此时sk和nsk相等，会进入
		tcp_rcv_state_process的处理流程*/
			sock_rps_save_rxhash(nsk, skb);
			if (tcp_child_process(sk, nsk, skb)) {
				rsk = nsk;
				goto reset;
			}
			return 0;
		}
	} else
		sock_rps_save_rxhash(sk, skb);
    
    再往下，由于syn是发给listensocket的，就走
    tcp_rcv_state_process->tcp_conn_request->tcp_v4_send_synack，回复syn的ack给客户端。
    客户端收到服务器回复的syn的ack之后，会完成第三次握手，发送ack，然后客户端的socket进入establish状态。注意，
    此时服务器端的连接socket还没有创建，还是listen socket来处理这个ack。
    服务器收到这个ack之后，走如下的调用链：
    
    函数调用链：tcp_v4_rcv-->tcp_v4_do_rcv-->tcp_v4_hnd_req-->tcp_check_req-->
tcp_v4_syn_recv_sock-->tcp_create_openreq_child
    我们展开tcp_v4_hnd_req看看它做了什么。
    static struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)
{
	struct tcphdr *th = tcp_hdr(skb);
	const struct iphdr *iph = ip_hdr(skb);
	struct sock *nsk;
	struct request_sock **prev;
	/* Find possible connection requests. */
	struct request_sock *req = inet_csk_search_req(sk, &prev, th->source,
						       iph->saddr, iph->daddr);
	if (req)/*如果这个ip，port对应的hash已经存在于syn_table中，返回该req，否则返回NULL*/
		return tcp_check_req(sk, skb, req, prev, false);/*req不为NULL，进入tcp_check_req流程*/

	nsk = inet_lookup_established(sock_net(sk), &tcp_hashinfo, iph->saddr,
			th->source, iph->daddr, th->dest, inet_iif(skb));

	if (nsk) {
		if (nsk->sk_state != TCP_TIME_WAIT) {
			bh_lock_sock(nsk);
			return nsk;/**/
		}
		inet_twsk_put(inet_twsk(nsk));
		return NULL;
	}

#ifdef CONFIG_SYN_COOKIES
	if (!th->syn)
		sk = cookie_v4_check(sk, skb, &(IPCB(skb)->opt));
#endif
	return sk;
}

继续查看tcp_check_req函数，由于该函数很长，我们略过检查的部分，进入syn的recv流程函数
inet_csk(sk)->icsk_af_ops->syn_recv_sock是一个回调，
const struct inet_connection_sock_af_ops ipv4_specific = {
	.queue_xmit	   = ip_queue_xmit,
	.send_check	   = tcp_v4_send_check,
	.rebuild_header	   = inet_sk_rebuild_header,
	.sk_rx_dst_set	   = inet_sk_rx_dst_set,
	.conn_request	   = tcp_v4_conn_request,
	.syn_recv_sock	   = tcp_v4_syn_recv_sock,/*其实就是调用的这个函数*/
	
而tcp_v4_syn_recv_sock的入口检查中：
	if (sk_acceptq_is_full(sk))
		goto exit_overflow;
static inline bool sk_acceptq_is_full(const struct sock *sk)
{
	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
}
而sk->sk_max_ack_backlog则是listen的时候，pn_socket_listen用户态带过来的参数。
所以，当backlog满了，则会出现处于SYN-RECEIVED状态的情况，虽然rr的backlog设置已经很大，但由于cpu的冲高，
导致accept并没有及时去获取。
  
  有一个地方需要注意：inet_csk_clone_lock函数给conn的socket设置状态为TCP_SYN_RECV，如果是
  sk_acceptq_is_full返回了，那么tcp_v4_syn_recv_sock按道理走不到
  tcp_create_openreq_child->inet_csk_clone_lock中来，也就不会设置状态为TCP_SYN_RECV，这就与我们能看到
  这个状态不符合，这个事因为ss和netstat看到的状态，和内核中sock的状态不是一一对应的。也就是说通过netstat 
  看到的套接字状态，并不是sock->sk_state字段，而是另外一个字段。
  tcp_rcv_state_process函数中，为针对TCP_SYN_RECV的connsock的状态，在下次收到客户端的第三次握手的ack之后，
  会调用tcp_ack检查该ack是否正常，如果正常，则将connsock的状态，由TCP_SYN_RECV迁移到
  tcp_set_state(sk, TCP_ESTABLISHED);
